{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('py36': conda)"
  },
  "interpreter": {
   "hash": "945f772f9badcbfc4a1df3d6527578ec95a0effa6aa5f833d2a8223f80199175"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pywt\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import initializers,layers\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "from keras import backend as K\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from time import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# disable GPU\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PIC_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ## Set GPU computing\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True \t   # to log device placment (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)\t# set this Tensorflow session as the default session for keras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def acc30(records,duration=30,sampling=0.02):\n",
    "    rec_num,length = records.shape\n",
    "    length = int(duration/sampling)\n",
    "    outrecs = np.zeros((rec_num,length),dtype=np.float32)\n",
    "    for i in range(rec_num):\n",
    "        irec = records[i,:]\n",
    "        pga_index = np.argmax(abs(irec))\n",
    "        if pga_index < length/2:\n",
    "            rec = irec[:length]\n",
    "        else:\n",
    "            rec = irec[(pga_index-int(length/2)):(pga_index+int(length/2)+(length%2 == 1))]\n",
    "        outrecs[i,:] = rec\n",
    "    return np.float32(outrecs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import pywt\n",
    "def wavelettrans(x,scales,duration=30,step=0.02, wavelet='morl'):\n",
    "    length = int(duration/step)\n",
    "    sz1,sz2 = x.shape\n",
    "    # wavelet = 'sym2'\n",
    "    outimages = np.zeros((sz1,len(scales),length,1),dtype=np.float32)\n",
    "    for i in range(sz1):\n",
    "        irec = x[i,:]\n",
    "        pga_index = np.argmax(abs(irec))\n",
    "        if pga_index < length/2:\n",
    "            rec = irec[:length]\n",
    "        else:\n",
    "            rec = irec[(pga_index-int(length/2)):(pga_index+int(length/2)+(length%2 == 1))]\n",
    "        cfs,_ = pywt.cwt(rec,scales,wavelet)\n",
    "        power = abs(cfs)\n",
    "        # cfs = np.around((cfs-cfs.min())/((cfs.max()-cfs.min())/255))\n",
    "        outimages[i,:,:,0] = power\n",
    "    return np.float32(outimages)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"data.pickle\",\"rb\") as f:\n",
    "    tr_data,ts_data = pickle.load(f)\n",
    "\n",
    "x_train = acc30(tr_data[:,1:])\n",
    "x_test = acc30(ts_data[:,1:])\n",
    "y_train = tr_data[:,0].astype(int)-1\n",
    "y_test = ts_data[:,0].astype(int)-1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\n",
    "start_time = time.monotonic()\n",
    "scales = np.arange(1,129)\n",
    "x_train = wavelettrans(x_train,scales,wavelet='morl')\n",
    "x_test = wavelettrans(x_test,scales,wavelet='morl')\n",
    "end_time = time.monotonic()\n",
    "print(timedelta(seconds=end_time-start_time))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(layers.Conv2D(64,(2,3), activation='relu', padding='same', kernel_regularizer='l2', input_shape=(x_train[1,:,:,:].shape)))\n",
    "model.add(layers.MaxPooling2D((2,2),padding='same'))\n",
    "model.add(layers.Conv2D(32,(2,3), activation='relu', padding='same', kernel_regularizer='l2'))\n",
    "model.add(layers.MaxPooling2D((2,2),padding='same'))\n",
    "model.add(layers.Conv2D(16,(2,3), activation='relu', padding='same', kernel_regularizer='l2'))\n",
    "model.add(layers.MaxPooling2D((2,2),padding='same'))\n",
    "model.add(layers.Conv2D(16,(2,3), activation='relu', padding='same', kernel_regularizer='l2'))\n",
    "model.add(layers.MaxPooling2D((2,2),padding='same'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history=[]\n",
    "final=[]\n",
    "a=np.zeros(100)\n",
    "b=np.zeros(100)\n",
    "\n",
    "optimizer = 'adam'\n",
    "# model = Model(input_img, category)\n",
    "model.compile(optimizer= optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "filepath = 'CPU-cnn2d30-best-up-{}.hdf5'.format(optimizer)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"val_loss\",verbose=1,save_best_only=True,mode=\"min\")\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# keras.initializers.he_uniform(seed=None)\n",
    "# start_time = time.monotonic()\n",
    "import time\n",
    "from datetime import timedelta\n",
    "start_time = time.monotonic()\n",
    "history.append(model.fit(x_train, y_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, y_test),\n",
    "                callbacks=callbacks_list,\n",
    "                verbose=0))\n",
    "end_time = time.monotonic()\n",
    "print(timedelta(seconds=end_time-start_time))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_time = time.monotonic()\n",
    "scores = model.evaluate(x_test,y_test,verbose=1)\n",
    "end_time = time.monotonic()\n",
    "print(timedelta(seconds=end_time-start_time))\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predicts = model.predict(x_test)\n",
    "y_pred = np.argmax(predicts,axis=1)\n",
    "tf.math.confusion_matrix(y_pred,y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(history[0].history['accuracy'])\n",
    "plt.plot(history[0].history['val_accuracy'])\n",
    "\n",
    "plt.title('Model Accuracy against Network Size')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['large_train', 'large_test','medium_train','medium_test','small_train','small_test'], loc='upper left')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(history[0].history['loss'])\n",
    "plt.plot(history[0].history['val_loss'])\n",
    "\n",
    "plt.title('Model Accuracy against Network Size')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['large_train', 'large_test','medium_train','medium_test','small_train','small_test'], loc='upper left')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"cnn2d-data-history.pkl\", \"wb\") as f:\n",
    "    pickle.dump([y_pred,history[0].history['accuracy'],history[0].history['val_accuracy'],\n",
    "    history[0].history['loss'],history[0].history['val_loss']],f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}